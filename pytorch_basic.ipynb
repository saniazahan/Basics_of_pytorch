{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_basic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0QUVUhA7rwa6KthILMdo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saniazahan/Basics_of_pytorch/blob/master/pytorch_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5oS_H9SFaeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_PavGXF_66p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0337f809-432a-4381-b5da-1aa324e88d4c"
      },
      "source": [
        "x = torch.rand(3)\n",
        "t1 = torch.tensor([1.0,5])\n",
        "print(t1)\n",
        "print(x)\n",
        "t1.shape\n",
        "\n",
        "y = np.array([[1,2],[2,3.]])\n",
        "p = torch.tensor(y)\n",
        "print(p)\n",
        "print(p.dtype)\n",
        "\n",
        "y2 = p.numpy()\n",
        "print(y2)\n",
        "\n",
        "x = torch.tensor(3.)\n",
        "y = torch.tensor(4.,requires_grad=True)\n",
        "z = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "w = x*y+z\n",
        "print(w)\n",
        "\n",
        "pp = w.backward()\n",
        "print('dw/dx',x.grad)\n",
        "print('dw/dy',y.grad)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 5.])\n",
            "tensor([0.2678, 0.0363, 0.9728])\n",
            "tensor([[1., 2.],\n",
            "        [2., 3.]], dtype=torch.float64)\n",
            "torch.float64\n",
            "[[1. 2.]\n",
            " [2. 3.]]\n",
            "tensor(17., grad_fn=<AddBackward0>)\n",
            "dw/dx None\n",
            "dw/dy tensor(3.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohwe2dSP9gNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "79bf9807-1bbf-41a3-d0bf-cdcaedc172f8"
      },
      "source": [
        "y = torch.tensor([[4.,5,9],[4.,7,1]], requires_grad=True)\n",
        "print(y)\n",
        "pp = y.backward\n",
        "print(pp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4., 5., 9.],\n",
            "        [4., 7., 1.]], requires_grad=True)\n",
            "<bound method Tensor.backward of tensor([[4., 5., 9.],\n",
            "        [4., 7., 1.]], requires_grad=True)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxvH6cCUVACu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b6e12942-3d90-4d08-83c7-c53c575c27dc"
      },
      "source": [
        "!git clone https://github.com/huguyuehuhu/HCN-pytorch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'HCN-pytorch'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 196 (delta 2), reused 0 (delta 0), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (196/196), 340.82 KiB | 17.04 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCw3exG4ZnQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d30d368b-1646-43c5-94e1-8612f7445fbc"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs,ys, epochs=500)\n",
        "print(model.predict([10.0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 59.6918\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 967us/step - loss: 47.3927\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 37.7075\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 30.0790\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 24.0688\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 19.3320\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 15.5972\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 12.6508\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 10.3249\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.4874\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.0343\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.8837\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.9713\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.2464\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.6693\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.2084\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.8392\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5423\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3024\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.1074\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9479\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8165\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7073\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.6156\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5379\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4713\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4135\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3628\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3178\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2773\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2405\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2067\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1754\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1461\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1185\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0923\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0674\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0435\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0205\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9983\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9769\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9561\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9358\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9161\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8969\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8782\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.8600\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8421\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8247\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8076\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 919us/step - loss: 0.7910\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7746\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7587\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7430\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 971us/step - loss: 0.7278\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.7128\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6981\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6838\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6697\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6559\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6425\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6293\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6163\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6037\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5913\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5791\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5672\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5556\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5441\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5330\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5220\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5113\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 972us/step - loss: 0.5008\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 982us/step - loss: 0.4905\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4804\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4706\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 939us/step - loss: 0.4609\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4514\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4422\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4331\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 989us/step - loss: 0.4242\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4155\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4069\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 963us/step - loss: 0.3986\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3904\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3824\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3745\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3668\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3593\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3519\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3447\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3376\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3307\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3239\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3172\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3107\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.3043\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2981\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2919\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2860\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2801\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2743\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2687\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2632\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2578\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2525\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2473\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2422\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2372\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2324\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2276\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2229\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2183\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2138\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2095\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2052\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2009\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1968\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1928\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1888\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1849\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1811\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1774\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1738\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1702\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1667\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1633\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1599\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1566\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 984us/step - loss: 0.1534\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1503\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1472\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1442\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1412\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1383\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1355\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1327\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1300\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1273\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1247\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1221\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1196\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.1171\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1147\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1124\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1101\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1078\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 993us/step - loss: 0.1056\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1034\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1013\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0992\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0972\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0952\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 827us/step - loss: 0.0932\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0913\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0894\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 869us/step - loss: 0.0876\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0858\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0840\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0823\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 884us/step - loss: 0.0806\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0790\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0773\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0758\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 913us/step - loss: 0.0742\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0727\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0712\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0697\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 994us/step - loss: 0.0683\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0669\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0655\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0642\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0628\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 906us/step - loss: 0.0616\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0603\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0591\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 827us/step - loss: 0.0578\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0567\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0555\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0544\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0532\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0521\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0511\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0500\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0490\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0480\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0470\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0460\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0451\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0442\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0433\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0424\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0415\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0406\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0398\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0390\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0382\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0374\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0366\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0359\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0351\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0344\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0337\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0330\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0323\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0317\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0310\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0304\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0298\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0292\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0286\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 871us/step - loss: 0.0280\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0274\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0268\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0263\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0257\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0252\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0242\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0237\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 792us/step - loss: 0.0232\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 894us/step - loss: 0.0227\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 819us/step - loss: 0.0223\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 887us/step - loss: 0.0214\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0209\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0205\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0201\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0193\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 841us/step - loss: 0.0189\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0181\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 877us/step - loss: 0.0177\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0174\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0170\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 799us/step - loss: 0.0167\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0163\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0160\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0156\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0153\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0150\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 739us/step - loss: 0.0147\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 858us/step - loss: 0.0144\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0141\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0138\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 956us/step - loss: 0.0135\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0133\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0130\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0127\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0125\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 949us/step - loss: 0.0122\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 955us/step - loss: 0.0119\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0117\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0115\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 815us/step - loss: 0.0112\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0110\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0108\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0105\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0103\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0101\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0099\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0095\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0091\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0089\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0088\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0086\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0084\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0082\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0081\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0079\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0077\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 909us/step - loss: 0.0076\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 956us/step - loss: 0.0074\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0073\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0071\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0070\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0068\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0067\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0061\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0060\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0059\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0058\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0057\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0055\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0054\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0053\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0052\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0051\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0050\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0049\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0048\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0040\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0039\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0036\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0035\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0031\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0030\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0030\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0029\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0029\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 866us/step - loss: 0.0028\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0027\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0027\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 792us/step - loss: 0.0025\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 994us/step - loss: 0.0024\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0023\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0023\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0022\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0022\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0021\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0021\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0020\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0020\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0020\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0019\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0019\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 976us/step - loss: 0.0016\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0015\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0015\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0013\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0012\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0012\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0011\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 983us/step - loss: 0.0011\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 999us/step - loss: 0.0010\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0010\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.8989e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.6956e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 801us/step - loss: 9.4965e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 868us/step - loss: 9.3014e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.1103e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 829us/step - loss: 8.9232e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 960us/step - loss: 8.7399e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.5604e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 978us/step - loss: 8.3845e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.2123e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 894us/step - loss: 8.0436e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 965us/step - loss: 7.8784e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 854us/step - loss: 7.7166e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.5581e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 838us/step - loss: 7.4028e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.2508e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.1019e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.9560e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 798us/step - loss: 6.8131e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.6731e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 791us/step - loss: 6.5361e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 995us/step - loss: 6.4018e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.2703e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.1415e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 968us/step - loss: 6.0154e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.8918e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 803us/step - loss: 5.7708e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.6522e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.5362e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.4224e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 926us/step - loss: 5.3110e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.2019e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0951e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.9904e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.8879e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 939us/step - loss: 4.7875e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 996us/step - loss: 4.6892e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.5929e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4985e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.4061e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3156e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2270e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.1401e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.0551e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.9718e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8902e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.8103e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7320e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.6554e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.5803e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.5068e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 968us/step - loss: 3.4347e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.3642e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.2951e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.2274e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.1611e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0962e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0326e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9703e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9093e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8495e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7910e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7336e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6775e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.6225e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 884us/step - loss: 2.5686e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.5159e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 980us/step - loss: 2.4642e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 999us/step - loss: 2.4135e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.3640e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.3154e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.2679e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.2213e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.1757e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.1310e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0872e-04\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 895us/step - loss: 2.0443e-04\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 2.0023e-04\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9612e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.9209e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8815e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 923us/step - loss: 1.8428e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.8050e-04\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7679e-04\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.7316e-04\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.6960e-04\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.6612e-04\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 895us/step - loss: 1.6271e-04\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5936e-04\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5609e-04\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5288e-04\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4974e-04\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 947us/step - loss: 1.4666e-04\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4365e-04\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4070e-04\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3781e-04\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3498e-04\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3221e-04\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2949e-04\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2684e-04\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2423e-04\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2168e-04\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1918e-04\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1673e-04\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1433e-04\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1198e-04\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.0968e-04\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0743e-04\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0522e-04\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0306e-04\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0094e-04\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.8870e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.6840e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.4850e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.2903e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.0994e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.9126e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 990us/step - loss: 8.7295e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5501e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.3744e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.2024e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.0339e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.8687e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.7072e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.5489e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 924us/step - loss: 7.3938e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.2419e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 7.0932e-05\n",
            "[[18.97543]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDjNfNnJbQFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d9b1c07-5f0d-459b-93c2-137547845690"
      },
      "source": [
        "print(model.predict([1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0074801]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8wWdhTdJnNF",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg5QJ8QaJmAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adB-Ak-mJ6Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input(temp, rainfall, humidity)\n",
        "inputs = np.array([[73,67,43],\n",
        "                   [91,88,64],\n",
        "                   [87,134,58],\n",
        "                   [102,143,37],\n",
        "                   [69,96,70]], dtype='float32')\n",
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56,70],\n",
        "                    [81,101],\n",
        "                    [119,133],\n",
        "                    [22,37],\n",
        "                    [103,119]], dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-isA_4X2LdzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "3c561632-ace6-4081-fe6e-f89b14567f48"
      },
      "source": [
        "# Convert inputs and targets to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102., 143.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zANcZ9tAOpUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weights and biases\n",
        "w = torch.rand(2, 3, requires_grad = True)\n",
        "b = torch.rand(2, requires_grad = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WUe3QRmSz0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model x * w + b\n",
        "def model(x):\n",
        "  return x @ w.t() + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b70zST5WTDUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6e0a08c7-d8b6-4c38-ffc7-cd9355fb0334"
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 60.9495, 136.7572],\n",
            "        [ 86.3140, 180.5627],\n",
            "        [ 82.4360, 191.1519],\n",
            "        [ 65.7661, 192.8620],\n",
            "        [ 88.0154, 166.9106]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CChFKvadpMyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MSE loss\n",
        "def mse(t1, t2):\n",
        "  diff = t1 - t2\n",
        "  return torch.sum(diff*diff) / diff.numel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJKtE53hpiYl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "507b9eb5-0a89-4dda-b297-774bb129f0e2"
      },
      "source": [
        "# Computer loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4428.6465, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpY19lmLqqQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e2f92c2c-7a1a-4d4f-d815-fdf2ecb927a0"
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()\n",
        "# Gradients for weights\n",
        "print(w)\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2087, 0.0602, 0.9687],\n",
            "        [0.9952, 0.4225, 0.8105]], requires_grad=True)\n",
            "tensor([[ 218.8054,  143.9409, -199.4726],\n",
            "        [7275.2915, 9230.8584, 4091.2041]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBQ1Ldlvtz61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1c3cb898-a27f-4b13-ed4a-06ec66750a09"
      },
      "source": [
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "  print(w)\n",
        "  print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2066, 0.0588, 0.9707],\n",
            "        [0.9225, 0.3301, 0.7696]], requires_grad=True)\n",
            "tensor([0.0231, 0.9507], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQNQAnIwizI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ef33107-7f4a-4cd6-9aa6-9a707858e8c8"
      },
      "source": [
        "for i in range(100):\n",
        "  preds = model(inputs)\n",
        "  # Computer loss\n",
        "  loss = mse(preds, targets)\n",
        "  # Compute gradients\n",
        "  loss.backward()\n",
        "  # Update weights\n",
        "  with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(878.4659, grad_fn=<DivBackward0>)\n",
            "tensor(869.9468, grad_fn=<DivBackward0>)\n",
            "tensor(863.7957, grad_fn=<DivBackward0>)\n",
            "tensor(859.0950, grad_fn=<DivBackward0>)\n",
            "tensor(855.2850, grad_fn=<DivBackward0>)\n",
            "tensor(852.0238, grad_fn=<DivBackward0>)\n",
            "tensor(849.1022, grad_fn=<DivBackward0>)\n",
            "tensor(846.3932, grad_fn=<DivBackward0>)\n",
            "tensor(843.8187, grad_fn=<DivBackward0>)\n",
            "tensor(841.3312, grad_fn=<DivBackward0>)\n",
            "tensor(838.9020, grad_fn=<DivBackward0>)\n",
            "tensor(836.5127, grad_fn=<DivBackward0>)\n",
            "tensor(834.1530, grad_fn=<DivBackward0>)\n",
            "tensor(831.8159, grad_fn=<DivBackward0>)\n",
            "tensor(829.4973, grad_fn=<DivBackward0>)\n",
            "tensor(827.1947, grad_fn=<DivBackward0>)\n",
            "tensor(824.9066, grad_fn=<DivBackward0>)\n",
            "tensor(822.6323, grad_fn=<DivBackward0>)\n",
            "tensor(820.3706, grad_fn=<DivBackward0>)\n",
            "tensor(818.1214, grad_fn=<DivBackward0>)\n",
            "tensor(815.8842, grad_fn=<DivBackward0>)\n",
            "tensor(813.6591, grad_fn=<DivBackward0>)\n",
            "tensor(811.4455, grad_fn=<DivBackward0>)\n",
            "tensor(809.2440, grad_fn=<DivBackward0>)\n",
            "tensor(807.0538, grad_fn=<DivBackward0>)\n",
            "tensor(804.8751, grad_fn=<DivBackward0>)\n",
            "tensor(802.7078, grad_fn=<DivBackward0>)\n",
            "tensor(800.5517, grad_fn=<DivBackward0>)\n",
            "tensor(798.4070, grad_fn=<DivBackward0>)\n",
            "tensor(796.2734, grad_fn=<DivBackward0>)\n",
            "tensor(794.1509, grad_fn=<DivBackward0>)\n",
            "tensor(792.0392, grad_fn=<DivBackward0>)\n",
            "tensor(789.9388, grad_fn=<DivBackward0>)\n",
            "tensor(787.8492, grad_fn=<DivBackward0>)\n",
            "tensor(785.7704, grad_fn=<DivBackward0>)\n",
            "tensor(783.7022, grad_fn=<DivBackward0>)\n",
            "tensor(781.6448, grad_fn=<DivBackward0>)\n",
            "tensor(779.5979, grad_fn=<DivBackward0>)\n",
            "tensor(777.5616, grad_fn=<DivBackward0>)\n",
            "tensor(775.5358, grad_fn=<DivBackward0>)\n",
            "tensor(773.5204, grad_fn=<DivBackward0>)\n",
            "tensor(771.5153, grad_fn=<DivBackward0>)\n",
            "tensor(769.5204, grad_fn=<DivBackward0>)\n",
            "tensor(767.5358, grad_fn=<DivBackward0>)\n",
            "tensor(765.5613, grad_fn=<DivBackward0>)\n",
            "tensor(763.5970, grad_fn=<DivBackward0>)\n",
            "tensor(761.6427, grad_fn=<DivBackward0>)\n",
            "tensor(759.6982, grad_fn=<DivBackward0>)\n",
            "tensor(757.7637, grad_fn=<DivBackward0>)\n",
            "tensor(755.8391, grad_fn=<DivBackward0>)\n",
            "tensor(753.9241, grad_fn=<DivBackward0>)\n",
            "tensor(752.0189, grad_fn=<DivBackward0>)\n",
            "tensor(750.1234, grad_fn=<DivBackward0>)\n",
            "tensor(748.2375, grad_fn=<DivBackward0>)\n",
            "tensor(746.3611, grad_fn=<DivBackward0>)\n",
            "tensor(744.4941, grad_fn=<DivBackward0>)\n",
            "tensor(742.6365, grad_fn=<DivBackward0>)\n",
            "tensor(740.7885, grad_fn=<DivBackward0>)\n",
            "tensor(738.9496, grad_fn=<DivBackward0>)\n",
            "tensor(737.1198, grad_fn=<DivBackward0>)\n",
            "tensor(735.2996, grad_fn=<DivBackward0>)\n",
            "tensor(733.4880, grad_fn=<DivBackward0>)\n",
            "tensor(731.6859, grad_fn=<DivBackward0>)\n",
            "tensor(729.8925, grad_fn=<DivBackward0>)\n",
            "tensor(728.1080, grad_fn=<DivBackward0>)\n",
            "tensor(726.3327, grad_fn=<DivBackward0>)\n",
            "tensor(724.5661, grad_fn=<DivBackward0>)\n",
            "tensor(722.8082, grad_fn=<DivBackward0>)\n",
            "tensor(721.0592, grad_fn=<DivBackward0>)\n",
            "tensor(719.3189, grad_fn=<DivBackward0>)\n",
            "tensor(717.5871, grad_fn=<DivBackward0>)\n",
            "tensor(715.8637, grad_fn=<DivBackward0>)\n",
            "tensor(714.1491, grad_fn=<DivBackward0>)\n",
            "tensor(712.4429, grad_fn=<DivBackward0>)\n",
            "tensor(710.7451, grad_fn=<DivBackward0>)\n",
            "tensor(709.0557, grad_fn=<DivBackward0>)\n",
            "tensor(707.3745, grad_fn=<DivBackward0>)\n",
            "tensor(705.7017, grad_fn=<DivBackward0>)\n",
            "tensor(704.0370, grad_fn=<DivBackward0>)\n",
            "tensor(702.3806, grad_fn=<DivBackward0>)\n",
            "tensor(700.7321, grad_fn=<DivBackward0>)\n",
            "tensor(699.0917, grad_fn=<DivBackward0>)\n",
            "tensor(697.4594, grad_fn=<DivBackward0>)\n",
            "tensor(695.8351, grad_fn=<DivBackward0>)\n",
            "tensor(694.2185, grad_fn=<DivBackward0>)\n",
            "tensor(692.6100, grad_fn=<DivBackward0>)\n",
            "tensor(691.0091, grad_fn=<DivBackward0>)\n",
            "tensor(689.4163, grad_fn=<DivBackward0>)\n",
            "tensor(687.8310, grad_fn=<DivBackward0>)\n",
            "tensor(686.2533, grad_fn=<DivBackward0>)\n",
            "tensor(684.6833, grad_fn=<DivBackward0>)\n",
            "tensor(683.1210, grad_fn=<DivBackward0>)\n",
            "tensor(681.5661, grad_fn=<DivBackward0>)\n",
            "tensor(680.0187, grad_fn=<DivBackward0>)\n",
            "tensor(678.4788, grad_fn=<DivBackward0>)\n",
            "tensor(676.9462, grad_fn=<DivBackward0>)\n",
            "tensor(675.4210, grad_fn=<DivBackward0>)\n",
            "tensor(673.9031, grad_fn=<DivBackward0>)\n",
            "tensor(672.3926, grad_fn=<DivBackward0>)\n",
            "tensor(670.8891, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnsnFu34ySOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cfe7223-6861-4eb5-e15b-7beb04f43270"
      },
      "source": [
        "# Testing\n",
        "test1 = np.array([73,67,43], dtype='float32')\n",
        "prediction = model(torch.from_numpy(test1))\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([59.8273, 76.5413], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0p04WtW0KFX",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression using pytorch builtin methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEc_52cfPAvH",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression for Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjyRUVac2i7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhVRYAGs0U_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download training dataset\n",
        "dataset = MNIST(root='data/', download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcmXcVmR4DiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b7b296e-1f77-4680-c963-204c57010a70"
      },
      "source": [
        "print(len(dataset))\n",
        "test_dataset = MNIST(root='data/', train=False)\n",
        "print(len(test_dataset))\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=L size=28x28 at 0x7FBE05343978>, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r5SwFga33PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "# without this line image will be shown in popup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7TftKaQ4yCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9969a6a5-89ae-4b06-8895-792c88058625"
      },
      "source": [
        "# plot an image\n",
        "image, label = dataset[0]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7U1YkSP682X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X50Gciv7GUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataset object and convert images into pytorch tensors\n",
        "dataset = MNIST(root='data/', \n",
        "                train=True, \n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9tV4YbW7kfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "c2e9c997-4b3a-467a-f597-0193940a3bb4"
      },
      "source": [
        "img_tensor, label = dataset[0]\n",
        "print(img_tensor.shape, label)\n",
        "print(img_tensor[:,10:15,10:15])\n",
        "print(torch.max(img_tensor), torch.min(img_tensor))\n",
        "\n",
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) 5\n",
            "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
            "tensor(1.) tensor(0.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbe04e07278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmO3cEu182hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# function returns split indices from n by % from val_pct\n",
        "def split_indices(n, val_pct):\n",
        "  # Determine the size of validation set\n",
        "  n_val = int(val_pct * n)\n",
        "  # Create random permutation of 0 to n-1\n",
        "  idx = np.random.permutation(n)\n",
        "  # Pick first n_val indices for validation set\n",
        "  return idx[n_val:], idx[:n_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaL7LbGS_iBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0df3c344-8e94-47d7-aebc-6e332ea34552"
      },
      "source": [
        "train_indices, val_indices = split_indices(len(dataset), val_pct = 0.2)\n",
        "print(len(train_indices))\n",
        "print(len(val_indices))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000\n",
            "12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFeei1ftM-4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils .data.dataloader import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeSpN3J9NUq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Training sampler and data loader\n",
        "train_sampler = SubsetRandomSampler((train_indices))\n",
        "train_loader = DataLoader(dataset,\n",
        "                          batch_size, sampler = train_sampler)\n",
        "\n",
        "# Validation sampler and data loader\n",
        "val_sampler = SubsetRandomSampler((val_indices))\n",
        "val_loader = DataLoader(dataset,\n",
        "                          batch_size, sampler = val_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pNzSAeBNKgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "95d3e901-1b6b-4212-a896-5a480216889e"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28 * 28\n",
        "num_classes = 10\n",
        "# Logistic Regression  model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "print(model.weight.shape)\n",
        "print(model.bias.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 784])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlaq9EvXESX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redefining model to flatten the image into tensor\n",
        "class MnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = xb.reshape(-1, 784)\n",
        "    out = self.linear(xb)\n",
        "    return out\n",
        "\n",
        "model = MnistModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwPJzu6MNKLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8c738cb4-e76d-496b-93da-cc59c0817d94"
      },
      "source": [
        "# Training model\n",
        "for images, labels in train_loader:\n",
        "  print(labels)\n",
        "  print(images.shape)\n",
        "  outputs = model(images)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 8, 6, 5, 2, 6, 8, 3, 4, 2, 1, 0, 3, 8, 9, 8, 8, 6, 4, 0, 1, 6, 1, 9,\n",
            "        6, 3, 6, 1, 9, 8, 4, 8, 3, 6, 3, 7, 2, 8, 1, 4, 4, 6, 0, 0, 7, 8, 7, 1,\n",
            "        2, 6, 4, 6, 3, 0, 6, 9, 6, 1, 3, 8, 5, 5, 0, 3, 9, 3, 8, 6, 9, 9, 3, 3,\n",
            "        1, 0, 8, 2, 8, 5, 6, 6, 7, 1, 9, 8, 4, 0, 9, 1, 1, 3, 9, 7, 3, 7, 0, 7,\n",
            "        6, 5, 0, 2])\n",
            "torch.Size([100, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gvSIkDpNJhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ebc85fbd-60fd-4274-b759-73863236d2b3"
      },
      "source": [
        "print(outputs.shape)\n",
        "print(outputs[2].data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 10])\n",
            "tensor([-0.2750,  0.1070, -0.0521, -0.0958,  0.2220, -0.0261, -0.2351,  0.3594,\n",
            "        -0.1126,  0.0165])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAqpwszSnjSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e45bc9b2-2e2b-4429-a68e-a2f86340c183"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Apply softmax for each output row\n",
        "probs  =F.softmax(outputs, dim = 1)\n",
        "\n",
        "# Look at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs.data)\n",
        "\n",
        "# Add up the probabilities of an output row\n",
        "print(\"Sum: \", torch.sum(probs[90]).item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample probabilities:\n",
            " tensor([[0.0523, 0.1198, 0.1076, 0.1312, 0.1047, 0.1168, 0.0904, 0.0814, 0.0999,\n",
            "         0.0958],\n",
            "        [0.0964, 0.1040, 0.0952, 0.1003, 0.0826, 0.0976, 0.0989, 0.1045, 0.1238,\n",
            "         0.0966],\n",
            "        [0.0753, 0.1103, 0.0941, 0.0901, 0.1238, 0.0966, 0.0784, 0.1420, 0.0886,\n",
            "         0.1008],\n",
            "        [0.1006, 0.1004, 0.1044, 0.1197, 0.0928, 0.1014, 0.0736, 0.1223, 0.0859,\n",
            "         0.0990],\n",
            "        [0.0587, 0.1454, 0.1169, 0.0938, 0.0697, 0.1314, 0.0817, 0.1055, 0.0951,\n",
            "         0.1020],\n",
            "        [0.0905, 0.1162, 0.1033, 0.1264, 0.0784, 0.1036, 0.1013, 0.1129, 0.0865,\n",
            "         0.0809],\n",
            "        [0.1000, 0.0929, 0.1145, 0.0858, 0.1012, 0.0987, 0.0742, 0.1348, 0.1098,\n",
            "         0.0879],\n",
            "        [0.0760, 0.1120, 0.0760, 0.1115, 0.0950, 0.1245, 0.0931, 0.1037, 0.1089,\n",
            "         0.0994],\n",
            "        [0.0945, 0.1226, 0.0795, 0.1623, 0.1031, 0.0891, 0.0686, 0.0857, 0.1016,\n",
            "         0.0930],\n",
            "        [0.0700, 0.1201, 0.1016, 0.1116, 0.1057, 0.1075, 0.0716, 0.0979, 0.1162,\n",
            "         0.0977],\n",
            "        [0.0668, 0.1032, 0.0994, 0.0979, 0.1149, 0.1071, 0.1317, 0.0997, 0.0914,\n",
            "         0.0878],\n",
            "        [0.0482, 0.1161, 0.1471, 0.1014, 0.1069, 0.1016, 0.1019, 0.0938, 0.0990,\n",
            "         0.0841],\n",
            "        [0.0710, 0.1012, 0.1033, 0.1255, 0.1145, 0.1083, 0.0909, 0.0886, 0.1032,\n",
            "         0.0934],\n",
            "        [0.0714, 0.0852, 0.0912, 0.1128, 0.1017, 0.0940, 0.1100, 0.1261, 0.1191,\n",
            "         0.0887],\n",
            "        [0.0735, 0.1121, 0.0831, 0.1115, 0.1244, 0.1107, 0.0887, 0.0993, 0.1000,\n",
            "         0.0968],\n",
            "        [0.0725, 0.1039, 0.1046, 0.1272, 0.1287, 0.1217, 0.0701, 0.0938, 0.1003,\n",
            "         0.0774],\n",
            "        [0.0876, 0.0940, 0.0850, 0.1024, 0.1099, 0.1259, 0.0858, 0.1031, 0.1100,\n",
            "         0.0964],\n",
            "        [0.0788, 0.1137, 0.0775, 0.1204, 0.0826, 0.1119, 0.1174, 0.1103, 0.0915,\n",
            "         0.0958],\n",
            "        [0.0877, 0.0902, 0.0803, 0.1192, 0.1159, 0.1157, 0.0993, 0.0895, 0.1136,\n",
            "         0.0885],\n",
            "        [0.0605, 0.1031, 0.1323, 0.1169, 0.0852, 0.1361, 0.1022, 0.0878, 0.0868,\n",
            "         0.0890],\n",
            "        [0.0894, 0.0953, 0.1255, 0.1201, 0.0921, 0.1046, 0.0699, 0.1015, 0.1103,\n",
            "         0.0913],\n",
            "        [0.0631, 0.1076, 0.1223, 0.1179, 0.0958, 0.1186, 0.0766, 0.1145, 0.0926,\n",
            "         0.0911],\n",
            "        [0.0959, 0.1022, 0.1178, 0.1197, 0.0834, 0.1103, 0.0732, 0.0972, 0.1072,\n",
            "         0.0930],\n",
            "        [0.0725, 0.1221, 0.0919, 0.1301, 0.1148, 0.0980, 0.0960, 0.1080, 0.0934,\n",
            "         0.0732],\n",
            "        [0.0643, 0.0964, 0.1058, 0.1223, 0.0824, 0.1258, 0.1060, 0.1257, 0.0845,\n",
            "         0.0869],\n",
            "        [0.0696, 0.0983, 0.0958, 0.1144, 0.0918, 0.1105, 0.0774, 0.1207, 0.1327,\n",
            "         0.0888],\n",
            "        [0.0649, 0.1170, 0.1073, 0.1105, 0.0998, 0.1089, 0.0760, 0.1267, 0.0951,\n",
            "         0.0938],\n",
            "        [0.0695, 0.1116, 0.0877, 0.1095, 0.1014, 0.1084, 0.1138, 0.1120, 0.0957,\n",
            "         0.0904],\n",
            "        [0.0908, 0.0767, 0.1011, 0.0975, 0.1338, 0.1076, 0.0813, 0.1248, 0.1046,\n",
            "         0.0819],\n",
            "        [0.0997, 0.1011, 0.1067, 0.1091, 0.1009, 0.0940, 0.0883, 0.0883, 0.1113,\n",
            "         0.1007],\n",
            "        [0.0807, 0.0904, 0.0816, 0.1048, 0.1284, 0.1264, 0.0849, 0.1068, 0.1138,\n",
            "         0.0821],\n",
            "        [0.1024, 0.0970, 0.0881, 0.1012, 0.1015, 0.1091, 0.0925, 0.1015, 0.1206,\n",
            "         0.0862],\n",
            "        [0.0788, 0.1007, 0.0796, 0.1127, 0.1077, 0.1159, 0.0991, 0.1118, 0.1094,\n",
            "         0.0843],\n",
            "        [0.0659, 0.0996, 0.1155, 0.1161, 0.1024, 0.1208, 0.0680, 0.1249, 0.0944,\n",
            "         0.0925],\n",
            "        [0.0699, 0.0803, 0.0720, 0.1183, 0.1244, 0.1088, 0.1107, 0.0965, 0.1229,\n",
            "         0.0962],\n",
            "        [0.0534, 0.1100, 0.0826, 0.1357, 0.0982, 0.0951, 0.1053, 0.1027, 0.1319,\n",
            "         0.0851],\n",
            "        [0.0672, 0.1238, 0.1003, 0.0961, 0.1127, 0.1126, 0.0988, 0.1005, 0.1031,\n",
            "         0.0850],\n",
            "        [0.0600, 0.1097, 0.0925, 0.1240, 0.1048, 0.0978, 0.0923, 0.1290, 0.1066,\n",
            "         0.0832],\n",
            "        [0.0773, 0.1099, 0.0875, 0.0963, 0.1300, 0.1002, 0.1004, 0.1174, 0.0806,\n",
            "         0.1004],\n",
            "        [0.0607, 0.0997, 0.0720, 0.1249, 0.1422, 0.1223, 0.0823, 0.1057, 0.1013,\n",
            "         0.0888],\n",
            "        [0.0591, 0.1135, 0.0718, 0.1226, 0.1370, 0.1062, 0.0804, 0.1197, 0.1007,\n",
            "         0.0891],\n",
            "        [0.0868, 0.0980, 0.0919, 0.1116, 0.0931, 0.0989, 0.1111, 0.1067, 0.1097,\n",
            "         0.0922],\n",
            "        [0.0661, 0.0880, 0.1231, 0.1125, 0.1158, 0.1329, 0.0944, 0.1040, 0.0820,\n",
            "         0.0812],\n",
            "        [0.0683, 0.0962, 0.1650, 0.1290, 0.0945, 0.0986, 0.1133, 0.0826, 0.0716,\n",
            "         0.0809],\n",
            "        [0.0831, 0.0926, 0.0744, 0.1180, 0.1298, 0.1205, 0.0969, 0.1013, 0.0999,\n",
            "         0.0836],\n",
            "        [0.0809, 0.1092, 0.1010, 0.1057, 0.1218, 0.1216, 0.0647, 0.1013, 0.1136,\n",
            "         0.0803],\n",
            "        [0.0558, 0.0848, 0.1069, 0.1095, 0.1032, 0.0979, 0.0826, 0.1277, 0.1305,\n",
            "         0.1011],\n",
            "        [0.0760, 0.1097, 0.1046, 0.1229, 0.0861, 0.1165, 0.1040, 0.1127, 0.0915,\n",
            "         0.0761],\n",
            "        [0.0799, 0.1052, 0.1143, 0.1071, 0.0922, 0.1034, 0.0896, 0.1098, 0.1052,\n",
            "         0.0934],\n",
            "        [0.0851, 0.1066, 0.1107, 0.0973, 0.0912, 0.0943, 0.0891, 0.1280, 0.0939,\n",
            "         0.1037],\n",
            "        [0.0805, 0.1410, 0.1475, 0.0993, 0.0738, 0.0841, 0.0709, 0.0951, 0.0917,\n",
            "         0.1160],\n",
            "        [0.0755, 0.1159, 0.0953, 0.1218, 0.0948, 0.0986, 0.0860, 0.1197, 0.0987,\n",
            "         0.0936],\n",
            "        [0.0629, 0.1077, 0.0856, 0.1446, 0.0805, 0.1196, 0.1039, 0.1011, 0.1044,\n",
            "         0.0898],\n",
            "        [0.0723, 0.0905, 0.1367, 0.1204, 0.0922, 0.0999, 0.0887, 0.1071, 0.1037,\n",
            "         0.0886],\n",
            "        [0.0789, 0.1156, 0.1132, 0.0953, 0.1090, 0.0778, 0.0816, 0.1267, 0.0960,\n",
            "         0.1059],\n",
            "        [0.0842, 0.1032, 0.0907, 0.1087, 0.1275, 0.0906, 0.1162, 0.0774, 0.1143,\n",
            "         0.0872],\n",
            "        [0.0670, 0.1040, 0.0926, 0.1694, 0.0688, 0.0897, 0.0798, 0.1120, 0.1115,\n",
            "         0.1052],\n",
            "        [0.0775, 0.1107, 0.0967, 0.0968, 0.1045, 0.0986, 0.1147, 0.1043, 0.0939,\n",
            "         0.1022],\n",
            "        [0.0540, 0.1143, 0.0838, 0.0969, 0.1439, 0.1312, 0.0771, 0.1210, 0.1072,\n",
            "         0.0706],\n",
            "        [0.0666, 0.0944, 0.0773, 0.1122, 0.1060, 0.1261, 0.0888, 0.1169, 0.1316,\n",
            "         0.0800],\n",
            "        [0.0740, 0.0922, 0.0874, 0.1511, 0.1240, 0.1345, 0.0926, 0.0697, 0.0961,\n",
            "         0.0785],\n",
            "        [0.0925, 0.1151, 0.0915, 0.1245, 0.1119, 0.1114, 0.1018, 0.0697, 0.0950,\n",
            "         0.0865],\n",
            "        [0.0514, 0.1117, 0.0950, 0.1336, 0.0923, 0.1219, 0.0888, 0.0998, 0.1124,\n",
            "         0.0929],\n",
            "        [0.0870, 0.0952, 0.0836, 0.1032, 0.0865, 0.1201, 0.1071, 0.1053, 0.1148,\n",
            "         0.0972],\n",
            "        [0.0747, 0.0889, 0.0723, 0.1142, 0.1274, 0.1208, 0.0925, 0.0886, 0.1230,\n",
            "         0.0978],\n",
            "        [0.0661, 0.1090, 0.0917, 0.1056, 0.1059, 0.1162, 0.0871, 0.1207, 0.1114,\n",
            "         0.0863],\n",
            "        [0.0725, 0.1066, 0.0903, 0.1097, 0.0940, 0.0931, 0.1052, 0.1277, 0.1030,\n",
            "         0.0980],\n",
            "        [0.0762, 0.1207, 0.0885, 0.1182, 0.0770, 0.1190, 0.0890, 0.1331, 0.0928,\n",
            "         0.0857],\n",
            "        [0.0706, 0.1200, 0.0796, 0.1279, 0.1239, 0.1047, 0.1025, 0.1046, 0.0985,\n",
            "         0.0678],\n",
            "        [0.1018, 0.0801, 0.0987, 0.0938, 0.1405, 0.1118, 0.0893, 0.0953, 0.1045,\n",
            "         0.0843],\n",
            "        [0.0735, 0.1128, 0.0819, 0.1273, 0.1230, 0.1096, 0.0876, 0.0966, 0.1124,\n",
            "         0.0753],\n",
            "        [0.0735, 0.1092, 0.0905, 0.0953, 0.1286, 0.1137, 0.0976, 0.1058, 0.0970,\n",
            "         0.0887],\n",
            "        [0.0695, 0.1197, 0.0811, 0.0922, 0.1184, 0.1100, 0.1249, 0.1048, 0.0890,\n",
            "         0.0904],\n",
            "        [0.0617, 0.1128, 0.1545, 0.0946, 0.0781, 0.0988, 0.0975, 0.1052, 0.1067,\n",
            "         0.0902],\n",
            "        [0.0675, 0.1087, 0.0873, 0.1383, 0.0822, 0.1241, 0.0896, 0.0835, 0.1184,\n",
            "         0.1006],\n",
            "        [0.0612, 0.1226, 0.1098, 0.1191, 0.0904, 0.1108, 0.0989, 0.1082, 0.0965,\n",
            "         0.0825],\n",
            "        [0.0503, 0.1188, 0.0779, 0.1295, 0.1150, 0.1266, 0.0765, 0.1146, 0.1103,\n",
            "         0.0805],\n",
            "        [0.0650, 0.0986, 0.1039, 0.1266, 0.1117, 0.1141, 0.0951, 0.1003, 0.0892,\n",
            "         0.0955],\n",
            "        [0.0666, 0.0891, 0.1210, 0.1252, 0.0948, 0.1022, 0.0666, 0.1284, 0.0970,\n",
            "         0.1091],\n",
            "        [0.0681, 0.1187, 0.1307, 0.1486, 0.0690, 0.1042, 0.0840, 0.0973, 0.0778,\n",
            "         0.1017],\n",
            "        [0.0719, 0.0824, 0.0990, 0.1148, 0.1209, 0.1181, 0.0942, 0.1190, 0.0980,\n",
            "         0.0818],\n",
            "        [0.0803, 0.1040, 0.1003, 0.1051, 0.0965, 0.1032, 0.1072, 0.1065, 0.0979,\n",
            "         0.0990],\n",
            "        [0.0718, 0.0811, 0.0878, 0.1200, 0.1364, 0.1224, 0.0895, 0.0783, 0.1202,\n",
            "         0.0926],\n",
            "        [0.0538, 0.0977, 0.1279, 0.1339, 0.0902, 0.1267, 0.0696, 0.0984, 0.0990,\n",
            "         0.1026],\n",
            "        [0.0982, 0.0965, 0.0954, 0.1279, 0.0954, 0.1054, 0.0645, 0.1024, 0.1139,\n",
            "         0.1005],\n",
            "        [0.0662, 0.1072, 0.0817, 0.1264, 0.0959, 0.0999, 0.0844, 0.0927, 0.1509,\n",
            "         0.0948],\n",
            "        [0.0599, 0.1060, 0.0735, 0.1243, 0.1140, 0.1072, 0.1090, 0.0884, 0.1203,\n",
            "         0.0975],\n",
            "        [0.0832, 0.1120, 0.0987, 0.1083, 0.0975, 0.0962, 0.1099, 0.1023, 0.0960,\n",
            "         0.0959],\n",
            "        [0.0810, 0.0968, 0.0917, 0.1050, 0.1042, 0.1025, 0.1187, 0.1083, 0.0949,\n",
            "         0.0969],\n",
            "        [0.0894, 0.1035, 0.0946, 0.1287, 0.0875, 0.1198, 0.1019, 0.1036, 0.0885,\n",
            "         0.0825],\n",
            "        [0.0613, 0.0847, 0.0942, 0.1118, 0.1180, 0.1264, 0.1010, 0.0894, 0.1212,\n",
            "         0.0920],\n",
            "        [0.0689, 0.0942, 0.0771, 0.1166, 0.1074, 0.1176, 0.1084, 0.1147, 0.1039,\n",
            "         0.0913],\n",
            "        [0.0882, 0.1005, 0.0940, 0.1098, 0.0847, 0.1117, 0.0866, 0.1143, 0.1221,\n",
            "         0.0879],\n",
            "        [0.0556, 0.1114, 0.0918, 0.1249, 0.0998, 0.1098, 0.0866, 0.1101, 0.1080,\n",
            "         0.1019],\n",
            "        [0.0701, 0.1135, 0.1646, 0.1420, 0.0737, 0.0935, 0.0986, 0.0753, 0.0705,\n",
            "         0.0982],\n",
            "        [0.0603, 0.0917, 0.0964, 0.0880, 0.1254, 0.1341, 0.0809, 0.1182, 0.1242,\n",
            "         0.0808],\n",
            "        [0.0899, 0.0998, 0.0924, 0.0968, 0.1062, 0.0900, 0.1039, 0.1127, 0.1028,\n",
            "         0.1055],\n",
            "        [0.0809, 0.1156, 0.1136, 0.0866, 0.0910, 0.0975, 0.0968, 0.1126, 0.0961,\n",
            "         0.1093],\n",
            "        [0.0414, 0.1022, 0.0922, 0.1336, 0.1024, 0.1348, 0.0991, 0.0834, 0.1169,\n",
            "         0.0941],\n",
            "        [0.0676, 0.1077, 0.1249, 0.1153, 0.0933, 0.1274, 0.0830, 0.1041, 0.0872,\n",
            "         0.0896]])\n",
            "Sum:  0.9999999403953552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsU-ybT_pKgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "54c1c2a6-5edf-43cb-d145-38c8c9b1b894"
      },
      "source": [
        "# Prediction\n",
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 8, 7, 7, 1, 3, 7, 5, 3, 1, 6, 2, 3, 7, 4, 4, 5, 3, 3, 5, 2, 2, 3, 3,\n",
            "        5, 8, 7, 6, 4, 8, 4, 8, 5, 7, 4, 3, 1, 7, 4, 4, 4, 3, 5, 2, 4, 4, 8, 3,\n",
            "        2, 7, 2, 3, 3, 2, 7, 4, 3, 6, 4, 8, 3, 3, 3, 5, 4, 7, 7, 7, 3, 4, 3, 4,\n",
            "        6, 2, 3, 1, 3, 3, 7, 3, 4, 6, 4, 3, 3, 8, 3, 1, 6, 3, 5, 5, 8, 3, 2, 5,\n",
            "        7, 1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcmPOrvIpcdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a67f96f1-b200-4a7c-c97f-deb38e157f4d"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 8, 6, 5, 2, 6, 8, 3, 4, 2, 1, 0, 3, 8, 9, 8, 8, 6, 4, 0, 1, 6, 1, 9,\n",
            "        6, 3, 6, 1, 9, 8, 4, 8, 3, 6, 3, 7, 2, 8, 1, 4, 4, 6, 0, 0, 7, 8, 7, 1,\n",
            "        2, 6, 4, 6, 3, 0, 6, 9, 6, 1, 3, 8, 5, 5, 0, 3, 9, 3, 8, 6, 9, 9, 3, 3,\n",
            "        1, 0, 8, 2, 8, 5, 6, 6, 7, 1, 9, 8, 4, 0, 9, 1, 1, 3, 9, 7, 3, 7, 0, 7,\n",
            "        6, 5, 0, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7WSgnzEuQuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(l1, l2):\n",
        "  return torch.sum(l1 == l2).item() / len(l1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbUyyE6ouhmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd21f109-3a60-406c-fa48-598299ca15fa"
      },
      "source": [
        "accuracy(preds, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWhKIOOjxE1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6081ac35-7af3-4e14-deb1-921382c89382"
      },
      "source": [
        "# Accuracy and max is not differentiable so use cross entropy \n",
        "loss_fn = F.cross_entropy\n",
        "\n",
        "# Loss for current batch of data\n",
        "loss = loss_fn(outputs, labels)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3289, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sROTRyXx8IbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWvfDHEO9d8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
        "  # Calculate loss\n",
        "  preds = model(xb)\n",
        "  loss = loss_func(preds, yb)\n",
        "\n",
        "  if opt is not None:\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    opt.step()\n",
        "    # Reset gradients\n",
        "    opt.zero_grad()\n",
        "\n",
        "  metric_result = None\n",
        "  if metric is not None:\n",
        "    # Compute the metric\n",
        "    metric_result = metric(preds, yb)\n",
        "\n",
        "  return loss.item(), len(xb), metric_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3iCQQH2LayW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
        "  with torch.no_grad():\n",
        "    # Pass each batch through the model\n",
        "    results = [loss_batch(model, loss_fn, xb, yb, metric=metric)\n",
        "               for xb, yb in valid_dl]\n",
        "    # Separate losses, counts and metrics\n",
        "    losses, nums, metrics = zip(*results)\n",
        "    # Total size of dataset\n",
        "    total = np.sum(nums)\n",
        "    # Avg. loss across dataset\n",
        "    avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
        "    avg_metric = None\n",
        "    if metric is not None:\n",
        "      # Avg. of metric across batches\n",
        "      avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
        "  return avg_loss, total, avg_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQDevbfM_Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "  _, preds = torch.max(outputs, dim = 1)\n",
        "  return torch.sum(preds == labels).item() / len(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPClu6pWNe4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cda1a84-544f-43d0-adf1-fa6e0b9aa420"
      },
      "source": [
        "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric=accuracy)\n",
        "print('Loss: {:.4f}, Accuracy: {:.4f},'.format(val_loss, val_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3209, Accuracy: 0.1192,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fq1n5t7OixS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric = None):\n",
        "    accuracies = []\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        for xb, yb in train_dl:\n",
        "            loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)\n",
        "        # Evaluation\n",
        "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
        "        val_loss, total, val_metric = result\n",
        "        accuracies += [val_metric]\n",
        "        # Print progress\n",
        "        if metric is None:\n",
        "           print('Epoch [{}/{}], Loss: {:.4f}'\n",
        "                .format(epoch+1, epochs, val_loss))\n",
        "        else:\n",
        "           print('Epoch [{}/{}, Loss: {:.4f}, {}: {:.4f}'\n",
        "                .format(epoch+1, epochs, val_loss, metric.__name__, val_metric))\n",
        "    return accuracies\n",
        "         \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmwEysTD5mq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redefine model and optimizer\n",
        "model = MnistModel() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgujSeKK6Ywy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "cc1666aa-9322-4a35-892d-b5ab6d3ebb44"
      },
      "source": [
        "accuracies = fit(45, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/45, Loss: 1.8931, accuracy: 0.6647\n",
            "Epoch [2/45, Loss: 1.5925, accuracy: 0.7468\n",
            "Epoch [3/45, Loss: 1.3811, accuracy: 0.7769\n",
            "Epoch [4/45, Loss: 1.2289, accuracy: 0.7966\n",
            "Epoch [5/45, Loss: 1.1160, accuracy: 0.8078\n",
            "Epoch [6/45, Loss: 1.0295, accuracy: 0.8164\n",
            "Epoch [7/45, Loss: 0.9614, accuracy: 0.8227\n",
            "Epoch [8/45, Loss: 0.9065, accuracy: 0.8290\n",
            "Epoch [9/45, Loss: 0.8611, accuracy: 0.8322\n",
            "Epoch [10/45, Loss: 0.8230, accuracy: 0.8358\n",
            "Epoch [11/45, Loss: 0.7906, accuracy: 0.8393\n",
            "Epoch [12/45, Loss: 0.7626, accuracy: 0.8425\n",
            "Epoch [13/45, Loss: 0.7382, accuracy: 0.8450\n",
            "Epoch [14/45, Loss: 0.7167, accuracy: 0.8472\n",
            "Epoch [15/45, Loss: 0.6976, accuracy: 0.8500\n",
            "Epoch [16/45, Loss: 0.6805, accuracy: 0.8524\n",
            "Epoch [17/45, Loss: 0.6651, accuracy: 0.8545\n",
            "Epoch [18/45, Loss: 0.6511, accuracy: 0.8560\n",
            "Epoch [19/45, Loss: 0.6384, accuracy: 0.8578\n",
            "Epoch [20/45, Loss: 0.6267, accuracy: 0.8585\n",
            "Epoch [21/45, Loss: 0.6160, accuracy: 0.8596\n",
            "Epoch [22/45, Loss: 0.6061, accuracy: 0.8612\n",
            "Epoch [23/45, Loss: 0.5969, accuracy: 0.8623\n",
            "Epoch [24/45, Loss: 0.5883, accuracy: 0.8630\n",
            "Epoch [25/45, Loss: 0.5804, accuracy: 0.8643\n",
            "Epoch [26/45, Loss: 0.5730, accuracy: 0.8655\n",
            "Epoch [27/45, Loss: 0.5660, accuracy: 0.8660\n",
            "Epoch [28/45, Loss: 0.5594, accuracy: 0.8670\n",
            "Epoch [29/45, Loss: 0.5533, accuracy: 0.8674\n",
            "Epoch [30/45, Loss: 0.5475, accuracy: 0.8684\n",
            "Epoch [31/45, Loss: 0.5419, accuracy: 0.8695\n",
            "Epoch [32/45, Loss: 0.5367, accuracy: 0.8704\n",
            "Epoch [33/45, Loss: 0.5317, accuracy: 0.8708\n",
            "Epoch [34/45, Loss: 0.5270, accuracy: 0.8713\n",
            "Epoch [35/45, Loss: 0.5226, accuracy: 0.8718\n",
            "Epoch [36/45, Loss: 0.5183, accuracy: 0.8719\n",
            "Epoch [37/45, Loss: 0.5142, accuracy: 0.8725\n",
            "Epoch [38/45, Loss: 0.5103, accuracy: 0.8733\n",
            "Epoch [39/45, Loss: 0.5065, accuracy: 0.8738\n",
            "Epoch [40/45, Loss: 0.5030, accuracy: 0.8745\n",
            "Epoch [41/45, Loss: 0.4995, accuracy: 0.8753\n",
            "Epoch [42/45, Loss: 0.4962, accuracy: 0.8759\n",
            "Epoch [43/45, Loss: 0.4931, accuracy: 0.8763\n",
            "Epoch [44/45, Loss: 0.4900, accuracy: 0.8768\n",
            "Epoch [45/45, Loss: 0.4871, accuracy: 0.8775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ZELlu7-6Lv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "1c00c121-df73-4331-fc5f-75b678447567"
      },
      "source": [
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy vs. No. of epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C5XSVS0_wDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88f6c21c-a910-4a26-d841-e29c6aef2106"
      },
      "source": [
        "print(accuracies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85425, 0.8555, 0.8573333333333333, 0.8586666666666667, 0.8595]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4uzQhK_xQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root = 'data/',\n",
        "                     train = False,\n",
        "                     transform = transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVOqC51pGcQG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "fcf28eb6-eeb8-4d97-dc30-efa50019e9ac"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap = 'gray')\n",
        "print('Shape:', img.shape)\n",
        "print('Label', label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([1, 28, 28])\n",
            "Label 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxGeh0elHPAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3599bd5a-d1a0-45e3-8433-00f02981f7c7"
      },
      "source": [
        " img.unsqueeze(0).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5w1xl1vNyo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicts image\n",
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim = 1)\n",
        "    return preds[0].item() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7mXK4q5OWXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7d8766a4-5fc7-4c16-aa1b-1312181ec05c"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap = 'gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DvIdJ6vPMGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02873561-23db-4a02-e1cd-555c8ff24b63"
      },
      "source": [
        "len(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDwcBmQxPhGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ac6b35c-3c1d-4060-d1c6-940cb1e7de4d"
      },
      "source": [
        " test_loader = DataLoader(test_dataset, batch_size = 200)\n",
        "\n",
        " test_loss, total, test_acc = evaluate(model, loss_fn, test_loader, metric = accuracy)\n",
        " print('Loss: {:.4f}, Accuracy: {:.4f}, Total Test Cases: {}'.format(test_loss, test_acc, total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.4573, Accuracy: 0.8886, Total Test Cases: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vb6ln4npxdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')\n",
        "\n",
        "# .state_dict (arranged as a dictionary) returns an OrderedDict containing all the weights and bias metrices "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doDEeIrexEj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "aac97742-de9f-4b0a-8f43-2fdd427a25ce"
      },
      "source": [
        "model2 = MnistModel()\n",
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0064,  0.0243, -0.0334,  ...,  0.0163, -0.0106, -0.0274],\n",
              "                      [ 0.0145,  0.0320,  0.0326,  ..., -0.0170,  0.0344, -0.0129],\n",
              "                      [-0.0194,  0.0060,  0.0197,  ..., -0.0168, -0.0052,  0.0176],\n",
              "                      ...,\n",
              "                      [-0.0021, -0.0239, -0.0070,  ..., -0.0323,  0.0346,  0.0107],\n",
              "                      [ 0.0172,  0.0132,  0.0167,  ...,  0.0268,  0.0291,  0.0035],\n",
              "                      [-0.0297, -0.0081, -0.0206,  ..., -0.0264,  0.0319,  0.0175]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0771,  0.1415, -0.0652, -0.0517,  0.0430,  0.1113,  0.0015,  0.0678,\n",
              "                      -0.2084, -0.0128]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0x5SygyyuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85f58df2-a511-4c14-ada2-dff51d21d545"
      },
      "source": [
        " # Sanity check to see model2 has same loss and accuracy as model\n",
        " test_loss, total, test_acc = evaluate(model2, loss_fn, test_loader, metric = accuracy)\n",
        " print('Loss: {:.4f}, Accuracy: {:.4f}, Total Test Cases: {}'.format(test_loss, test_acc, total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.4573, Accuracy: 0.8886, Total Test Cases: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVQdY5E9zFzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}